{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Two-Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            dJ_dz2 = -2*(y - a3)*a3*(1-a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(a2*(1-a2))\n",
    "                         \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ a1[np.newaxis,:] \n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom MLP Implementation\n",
    "\n",
    "\n",
    "> TO DO\n",
    "* Multilayer gradient\n",
    "* Weight initialization\n",
    "* Update fit function for new gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(TwoLayerPerceptronVectorized):\n",
    "    def __init__(self, nlayers=3, phi='sigmoid', cost='quadratic', **kwds):\n",
    "        \n",
    "        self.nlayers = nlayers\n",
    "        self.phi_func = phi\n",
    "        self.cost_func = cost\n",
    "        \n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        sum_ = 0\n",
    "        for w in weights:\n",
    "            sum_ += np.mean(w[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(sum_)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(x):\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        _weights = []\n",
    "        for i in range(self.nlayers-1):\n",
    "            \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "            if i == 0:              \n",
    "                W1 = np.random.randn(self.n_hidden, self.n_features_ + 1)\n",
    "                W1[:,1:] = W1[:,1:]/np.sqrt(self.n_features_+1) # don't saturate the neuron\n",
    "                W1[:,:1] = 0 # common practice to start with zero bias\n",
    "                _weights.append(W1)\n",
    "                continue\n",
    "            W1 = np.random.randn(self.n_hidden, _weights[i-1].shape[1] + 1)\n",
    "            W1[:,1:] = W1[:,1:]/np.sqrt(self.n_features_+1) # don't saturate the neuron\n",
    "            W1[:,:1] = 0 # common practice to start with zero bias\n",
    "            _weights.append(W1)\n",
    "            \n",
    "    \n",
    "        '''initialize weights for output layer'''\n",
    "        W2 = np.random.randn(self.n_output_, self.n_hidden + 1)\n",
    "        W2[:,1:] = W2[:,1:]/np.sqrt(_weights[i-1].shape[1]+1) # don't saturate the neuron\n",
    "        W2[:,:1] = 0 # common practice to start with zero bias\n",
    "        _weights.append(W2)\n",
    "        \n",
    "        return _weights\n",
    "\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \n",
    "        _layers = [] # list of A matrices\n",
    "        \n",
    "        '''set phi function'''\n",
    "        if self.phi_func == 'sigmoid':\n",
    "            _phi = self._sigmoid\n",
    "        elif seld.phi_func == 'linear':\n",
    "            _phi = self._linear\n",
    "            \n",
    "        \n",
    "        for i in range(self.nlayers+1):\n",
    "            #compute first layer\n",
    "            if i == 0:\n",
    "                A1 = self._add_bias_unit(X.T, how='row')\n",
    "                _layers.append(A1)\n",
    "                print(\"A{} shape:{}\".format(i+1, A1.shape))\n",
    "                continue\n",
    "                \n",
    "            #compute ith layer\n",
    "            A_prev = _layers[i-1]\n",
    "            W_prev = weights[i-1]\n",
    "            \n",
    "            print(\"W{} shape:{}\".format(i+1, W_prev.shape))\n",
    "            Z_prev = W_prev @ A_prev\n",
    "            print(\"Z{} shape:{}\".format(i+1, Z_prev.shape))\n",
    "            A_i = _phi(Z_prev)\n",
    "            print(\"A{} shape:{}\".format(i+1, A_i.shape))\n",
    "            \n",
    "            #add bias, if layer isn't output layer\n",
    "            if i < self.nlayers:\n",
    "                A_i = self._add_bias_unit(A_i, how='row')\n",
    "\n",
    "            #add layer to _layers\n",
    "            _layers.append(A_i)\n",
    "            \n",
    "            \n",
    "        return _layers\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]                \n",
    "        self.weights = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            layers = self._feedforward(X_data, self.weights)\n",
    "\n",
    "            cost = self._cost(layers[self.nlayers],Y_enc, self.weights)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grads = self._get_gradient(layers=layers, Y_enc=Y_enc, weights=self.weights)\n",
    "\n",
    "            #update weights\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.eta * grads[i]\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        layers= self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(layers[self.nlayers], axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def _cost(self, Af, Y_enc, weights):\n",
    "        '''Get the objective function value'''\n",
    "        \n",
    "        if self.cost_func == 'quadratic':            \n",
    "            cost = np.mean((Y_enc-Af)**2)\n",
    "            L2_term = self._L2_reg(self.l2_C, weights)\n",
    "            \n",
    "        elif self.cost_func == 'cross_entropy':\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(Af)+(1-Y_enc)*np.log(1-Af))))\n",
    "            L2_term = self._L2_reg(self.l2_C, weights)\n",
    "            \n",
    "        return cost + L2_term\n",
    "\n",
    "    \n",
    "    def _get_gradient(self, layers, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation for each layer\n",
    "        \"\"\"\n",
    "        _V = []\n",
    "        _grads = []\n",
    "        \n",
    "        '''calculate sensitivity at final layer'''\n",
    "        #quadratic cost\n",
    "        if self.cost_func == 'quadratic':\n",
    "            Vf = -2 * (Y_enc-layers[self.nlayers])*layers[self.nlayers]*(1-layers[self.nlayers])\n",
    "            \n",
    "        #cross entropy cost   \n",
    "        elif self.cost_func == 'cross_entropy':\n",
    "            Vf = (layers[self.nlayers] - Y_enc) \n",
    "            \n",
    "        '''calculate gradient for wrt last layer'''\n",
    "        grad_f = Vf @ layers[self.nlayers-1].T\n",
    "\n",
    "        #regularize gradient that are not bias\n",
    "        grad_f[:,1:] += weights[self.nlayers-1][:, 1:] * self.l2_C\n",
    "        \n",
    "        _V.append(Vf)\n",
    "        _grads.append(grad_f)\n",
    "        \n",
    "        '''backpropagate through L-1 layers to calculate sensitivities''' \n",
    "        for i in range(self.nlayers-2,-1,-1):\n",
    "            \n",
    "            A_i = layers[i+1]  #A_i+1\n",
    "            W_i = weights[i+1] #W_i+1\n",
    "            V_i2 = _V[0]       #uses sensitivity of layer (i+1)\n",
    "            \n",
    "            #calculate sensitivity for layer i\n",
    "            V_i = A_i*(1-A_i)*(W_i.T @ V_i2)\n",
    "            \n",
    "            #insert sensitivity for layer i at front of list\n",
    "            _V.insert(0,V_i)\n",
    "            \n",
    "            #calculate gradient for layer i\n",
    "            grad_i = V_i[1:, :] @ layers[i].T\n",
    "            \n",
    "            #regularize weights that are not bias terms\n",
    "            grad_i[:, 1:] += weights[i][:, 1:]\n",
    "            \n",
    "            _grads.insert(0,grad_i)\n",
    "        \n",
    "        return _grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = dict(n_hidden=30, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=200, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              nlayers=3,\n",
    "              phi='sigmoid',\n",
    "              cost='quadratic',\n",
    "              )\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptron(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 shape:(65, 1437)\n",
      "W2 shape:(30, 65)\n",
      "Z2 shape:(30, 1437)\n",
      "A2 shape:(30, 1437)\n",
      "W3 shape:(30, 66)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 31 is different from 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-54a66dad2e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-e60229d94dc2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# feedforward all instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-e60229d94dc2>\u001b[0m in \u001b[0;36m_feedforward\u001b[0;34m(self, X, weights)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W{} shape:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mZ_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_prev\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z{} shape:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mA_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 31 is different from 66)"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train, print_progress=True)\n",
    "yhat = mlp.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 31)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
