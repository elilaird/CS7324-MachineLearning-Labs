{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Two-Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            dJ_dz2 = -2*(y - a3)*a3*(1-a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(a2*(1-a2))\n",
    "                         \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ a1[np.newaxis,:] \n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom MLP Implementation\n",
    "\n",
    "\n",
    "> TO DO\n",
    "* Multilayer gradient\n",
    "* Weight initialization\n",
    "* Update fit function for new gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(TwoLayerPerceptronVectorized):\n",
    "    def __init__(self, nlayers=3, phi='sigmoid', cost='quadratic', **kwds):\n",
    "        \n",
    "        self.nlayers = nlayers\n",
    "        self.phi = phi\n",
    "        self.cost = cost\n",
    "        \n",
    "        super.__init__(**kwds)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        _weights = []\n",
    "        for i in range(nlayers-1):\n",
    "            \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "            W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "            _weights.append(W1)\n",
    "    \n",
    "        #initialize weights for output layer\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        _weights.append(W2)\n",
    "        \n",
    "        return _weights\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \n",
    "        _layers = [] # list of A matrices\n",
    "        \n",
    "        for i in range(nlayers):\n",
    "            #compute first layer\n",
    "            if i == 0:\n",
    "                A1 = self._add_bias_unit(X.T, how='row')\n",
    "                _layers.append(A1)\n",
    "                continue\n",
    "                \n",
    "            #compute ith layer\n",
    "            A_prev = _layers[i-1]\n",
    "            W_prev = weights[i-1]\n",
    "            \n",
    "            Z_prev = W_prev @ A_prev\n",
    "            A_i = self.sigmoid(Z_prev)\n",
    "            \n",
    "            #add bias, if layer isn't output layer\n",
    "            if i < nlayers-1:\n",
    "                A_i = self._add_bias_unit(A_i, how='row')\n",
    "            \n",
    "            #add layer to _layers\n",
    "            _layers.append(A_i)\n",
    "            \n",
    "            \n",
    "            return _layers\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]                \n",
    "        self.weights = self.initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            layers = self._feedforward(X_data, self.weights)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(layers=layers, Y_enc=Y_enc, weights=self.weights)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, layers, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation for each layer\n",
    "        \"\"\"\n",
    "        \n",
    "        #loop through each layer\n",
    "        for i in range(nlayers-1,-1,-1):\n",
    "            '''Compute gradient for ith layer wrt previous layer'''\n",
    "            \n",
    "            #sensitivities\n",
    "            A3 = layers[i]\n",
    "            V2 = -2*(Y_enc-layers[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
