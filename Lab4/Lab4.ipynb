{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Two-Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        grad1 = np.zeros(W1.shape)\n",
    "        grad2 = np.zeros(W2.shape)\n",
    "        \n",
    "        # for each instance's activations \n",
    "        for (a1,a2,a3,y) in zip(A1.T,A2.T,A3.T,Y_enc.T):\n",
    "            dJ_dz2 = -2*(y - a3)*a3*(1-a3)\n",
    "            dJ_dz1 = dJ_dz2 @ W2 @ np.diag(a2*(1-a2))\n",
    "                         \n",
    "            grad2 += dJ_dz2[:,np.newaxis]  @ a2[np.newaxis,:]\n",
    "            grad1 += dJ_dz1[1:,np.newaxis] @ a1[np.newaxis,:] \n",
    "            # don't incorporate bias term in the z1 gradient \n",
    "            # (its added in a2 from another layer)\n",
    "            \n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom MLP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(TwoLayerPerceptronVectorized):\n",
    "    def __init__(self, nlayers=3, phi='sigmoid', cost='quadratic', **kwds):\n",
    "        \n",
    "        self.nlayers = nlayers\n",
    "        self.phi_func = phi\n",
    "        self.cost_func = cost\n",
    "        self.alpha_ = 1.6732632423543772848170429916717\n",
    "        self.lambda_ = 1.0507009873554804934193349852946\n",
    "        \n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        sum_ = 0\n",
    "        for w in weights:\n",
    "            sum_ += np.mean(w[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(sum_)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(Z):\n",
    "        return np.maximum(0,Z.copy())\n",
    "    \n",
    "    @staticmethod\n",
    "    def _selu(Z):\n",
    "        return  Z * expit(Z)    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        _weights = []\n",
    "        for i in range(self.nlayers-1):\n",
    "            \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "            if i == 0:              \n",
    "                W1 = np.random.randn(self.n_hidden, self.n_features_ + 1)\n",
    "                W1[:,1:] = W1[:,1:]/np.sqrt(self.n_features_+1) # don't saturate the neuron\n",
    "                W1[:,:1] = 0 # common practice to start with zero bias\n",
    "                _weights.append(W1)\n",
    "                continue\n",
    "            W1 = np.random.randn(self.n_hidden, _weights[i-1].shape[0] + 1)\n",
    "            W1[:,1:] = W1[:,1:]/np.sqrt(_weights[i-1].shape[0]+1) # don't saturate the neuron\n",
    "            W1[:,:1] = 0 # common practice to start with zero bias\n",
    "            _weights.append(W1)\n",
    "            \n",
    "    \n",
    "        '''initialize weights for output layer'''\n",
    "        W2 = np.random.randn(self.n_output_, self.n_hidden + 1)\n",
    "        W2[:,1:] = W2[:,1:]/np.sqrt(self.n_hidden + 1) # don't saturate the neuron\n",
    "        W2[:,:1] = 0 # common practice to start with zero bias\n",
    "        _weights.append(W2)\n",
    "        \n",
    "        return _weights\n",
    "\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \n",
    "        _layers = [] # list of A matrices\n",
    "        \n",
    "        '''set phi function'''\n",
    "        if self.phi_func == 'sigmoid':\n",
    "            _phi = self._sigmoid\n",
    "        elif self.phi_func == 'linear':\n",
    "            _phi = self._linear\n",
    "        elif self.phi_func == 'relu':\n",
    "            _phi = self._relu\n",
    "        elif self.phi_func == 'selu':\n",
    "            _phi = self._selu\n",
    "            \n",
    "        \n",
    "        for i in range(self.nlayers+1):\n",
    "            #compute first layer\n",
    "            if i == 0:\n",
    "                A1 = self._add_bias_unit(X.T, how='row')\n",
    "                _layers.append(A1)\n",
    " \n",
    "                continue\n",
    "                \n",
    "            #compute ith layer\n",
    "            A_prev = _layers[i-1]\n",
    "            W_prev = weights[i-1]\n",
    "            \n",
    " \n",
    "            Z_prev = W_prev @ A_prev\n",
    "\n",
    "            A_i = _phi(Z_prev)\n",
    "  \n",
    "            \n",
    "            #add bias, if layer isn't output layer\n",
    "            if i < self.nlayers:\n",
    "                A_i = self._add_bias_unit(A_i, how='row')\n",
    "                \n",
    "            #use sigmoid for output layer if using relu\n",
    "            if self.phi_func == 'relu' and i == self.nlayers:\n",
    "                A_i = self._sigmoid(Z_prev)\n",
    "\n",
    "            #add layer to _layers\n",
    "            _layers.append(A_i)\n",
    "            \n",
    "            \n",
    "        return _layers\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]                \n",
    "        self.weights = self._initialize_weights()\n",
    "        \n",
    "        self.grad_avgs = [np.zeros(self.epochs) for _ in range(len(self.weights))] \n",
    "        self.grad3 = np.zeros(self.epochs)\n",
    "        self.grad1 = np.zeros(self.epochs)\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            layers = self._feedforward(X_data, self.weights)\n",
    "            cost = self._cost(layers[self.nlayers],Y_enc, self.weights)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grads = self._get_gradient(layers=layers, Y_enc=Y_enc, weights=self.weights)\n",
    "                \n",
    "            for k in range(len(grads)):\n",
    "                self.grad_avgs[k][i] = np.mean(grads[k])\n",
    "                \n",
    "            self.grad3[i] = np.mean(grads[2])\n",
    "            self.grad1[i] = np.mean(grads[0])\n",
    "            #update weights and save avg gradients\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j] -= self.eta * grads[j]\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        layers= self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(layers[self.nlayers], axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def _cost(self, Af, Y_enc, weights):\n",
    "        '''Get the objective function value'''\n",
    "        \n",
    "        if self.cost_func == 'quadratic':            \n",
    "            cost = np.mean((Y_enc-Af)**2)\n",
    "            L2_term = self._L2_reg(self.l2_C, weights)\n",
    "            \n",
    "        elif self.cost_func == 'cross_entropy':\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(Af)+(1-Y_enc)*np.log(1-Af))))\n",
    "            L2_term = self._L2_reg(self.l2_C, weights)\n",
    "            \n",
    "        return cost + L2_term\n",
    "\n",
    "    \n",
    "    def _get_gradient(self, layers, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation for each layer\n",
    "        \"\"\"\n",
    "        _V = []\n",
    "        _grads = []\n",
    "        \n",
    "        '''calculate sensitivity at final layer'''\n",
    "        #quadratic cost\n",
    "        if self.cost_func == 'quadratic':\n",
    "            Vf = -2 * (Y_enc-layers[self.nlayers])*layers[self.nlayers]*(1-layers[self.nlayers])\n",
    "            \n",
    "        #cross entropy cost   \n",
    "        elif self.cost_func == 'cross_entropy':\n",
    "            Vf = (layers[self.nlayers] - Y_enc) \n",
    "            \n",
    "        #calculate gradient for wrt last layer\n",
    "        grad_f = Vf @ layers[self.nlayers-1].T\n",
    "\n",
    "        #regularize gradient that are not bias\n",
    "        grad_f[:,1:] += weights[self.nlayers-1][:, 1:] * self.l2_C\n",
    "        \n",
    "        _V.append(Vf)\n",
    "        _grads.append(grad_f)\n",
    "        \n",
    "        '''backpropagate through L-1 layers to calculate sensitivities''' \n",
    "        for i in range(self.nlayers-2,-1,-1):\n",
    "   \n",
    "            A_i = layers[i+1]  #A_i+1\n",
    "            W_i = weights[i+1] #W_i+1\n",
    "            V_i2 = _V[0]       #uses sensitivity of layer (i+1)\n",
    "            \n",
    "            if self.phi_func == 'relu':\n",
    "                V_i = (W_i.T[1:,:] @ V_i2) \n",
    "                V_i[A_i[1:,:]<=0] = 0\n",
    "          \n",
    "            else:   \n",
    "                #calculate sensitivity for layer i\n",
    "                V_i = A_i[1:,:]*(1-A_i[1:,:])*(W_i.T[1:,:] @ V_i2)\n",
    "\n",
    "            #insert sensitivity for layer i at front of list\n",
    "            _V.insert(0,V_i)\n",
    "            \n",
    "            #calculate gradient for layer i\n",
    "            grad_i = V_i @ layers[i].T\n",
    "            \n",
    "            #regularize weights that are not bias terms\n",
    "            grad_i += weights[i]\n",
    "            \n",
    "            _grads.insert(0,grad_i)\n",
    "        \n",
    "        return _grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5   , -0.5   , -0.0625, ..., -0.1875, -0.5   , -0.5   ],\n",
       "       [-0.5   , -0.5   ,  0.    , ..., -0.5   , -0.5   , -0.5   ],\n",
       "       [-0.5   , -0.5   , -0.25  , ...,  0.25  , -0.4375, -0.5   ],\n",
       "       ...,\n",
       "       [-0.5   , -0.5   , -0.0625, ..., -0.5   , -0.5   , -0.5   ],\n",
       "       [-0.5   , -0.5   , -0.1875, ...,  0.1875, -0.3125, -0.5   ],\n",
       "       [-0.5   , -0.5   , -0.3125, ...,  0.    , -0.375 , -0.5   ]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:158: RuntimeWarning: invalid value encountered in log\n",
      "Epoch: 300/300"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7944444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "params = dict(n_hidden=80, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=300, # iterations\n",
    "              eta= 0.0001,  # learning rate\n",
    "              random_state=1,\n",
    "              nlayers=4,\n",
    "              phi='selu',\n",
    "              cost='cross_entropy',\n",
    "              )\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=True)\n",
    "yhat = mlp.predict(X_test)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test,yhat))\n",
    "#print(\"Macro F1 Score:\", f1_score(y_test,yhat, average='macro', labels=[1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6, 7])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)\n",
    "np.unique(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "base_params = dict( \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "            )\n",
    "\n",
    "\n",
    "phi_list = ['sigmoid','linear']\n",
    "cost_list = ['quadratic','cross_entropy']\n",
    "epochs_list = [200, 400, 500]\n",
    "n_layers_list = [2,3,4,5]\n",
    "n_neurons_list = [30,50,80]\n",
    "etas = [0.001, 1e-5]\n",
    "\n",
    "\n",
    "\n",
    "param_list = []\n",
    "for epoch in epochs_list:\n",
    "    for phi in phi_list:\n",
    "        for cost in cost_list:\n",
    "            for n_layer in n_layers_list:\n",
    "                for n_neurons in n_neurons_list:\n",
    "                    for eta in etas:\n",
    "                        params = base_params.copy()\n",
    "                        params['eta'] = eta\n",
    "                        params['phi'] = phi\n",
    "                        params['cost'] = cost\n",
    "                        params['epochs'] = epoch\n",
    "                        params['nlayers'] = n_layer\n",
    "                        params['n_hidden'] = n_neurons\n",
    "                        param_list.append(params)\n",
    "print(len(param_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.875\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.16666666666666666\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.8972222222222223\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.2111111111111111\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.9027777777777778\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 2 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.2222222222222222\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.33611111111111114\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06111111111111111\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.42777777777777776\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.24722222222222223\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 3 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06666666666666667\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.10277777777777777\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 4 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.08333333333333333\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.08333333333333333\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: quadratic | N_layers: 5 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.9472222222222222\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.19722222222222222\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.95\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.2833333333333333\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.9416666666666667\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 2 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.3472222222222222\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.9333333333333333\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.9361111111111111\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.7888888888888889\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.36944444444444446\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.08055555555555556\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 50 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 50 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 80 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 80 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.06944444444444445\n",
      "Activation: linear | Objective Function: quadratic | N_layers: 2 | N_hidden: 30 | Eta: 0.001 | Epochs: 200 | F1_Score: 0.08333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:93: RuntimeWarning: overflow encountered in matmul\n",
      "/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in square\n",
      "/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:180: RuntimeWarning: invalid value encountered in matmul\n",
      "/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:205: RuntimeWarning: overflow encountered in multiply\n",
      "/Users/eli/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:211: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: linear | Objective Function: quadratic | N_layers: 2 | N_hidden: 30 | Eta: 1e-05 | Epochs: 200 | F1_Score: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "#evaluate model on each param combination\n",
    "cost_param = []\n",
    "for params in param_list[:50]:\n",
    "    mlp = MultilayerPerceptron(**params)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    yhat = mlp.predict(X_test)\n",
    "    cost = accuracy_score(y_test,yhat)#f1_score(y_test, yhat, average='macro', labels=[1,2,3,4,5,6,7,8,9])\n",
    "    cost_param.append((cost, params))\n",
    "    print(\"Activation: {} | Objective Function: {} | N_layers: {} | N_hidden: {} | Eta: {} | Epochs: {} | F1_Score: {}\".format(\\\n",
    "                                                                                                params['phi'],\\\n",
    "                                                                                                params['cost'],\\\n",
    "                                                                                                params['nlayers'],\\\n",
    "                                                                                                params['n_hidden'],\\\n",
    "                                                                                                params['eta'],\\\n",
    "                                                                                                params['epochs'],\\\n",
    "                                                                                                cost\n",
    "                                                                                                ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Performing Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 50 | Epochs: 200 | F1_Score: 0.930038320711059\n",
      "2. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 3 | N_hidden: 80 | Epochs: 200 | F1_Score: 0.7526917195662501\n",
      "3. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 30 | Epochs: 200 | F1_Score: 0.32670738588661125\n",
      "4. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 50 | Epochs: 200 | F1_Score: 0.033905182142087306\n",
      "5. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 4 | N_hidden: 80 | Epochs: 200 | F1_Score: 0.014430014430014432\n",
      "6. Activation: sigmoid | Objective Function: cross_entropy | N_layers: 5 | N_hidden: 30 | Epochs: 200 | F1_Score: 0.014430014430014432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost_param.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "for i,key in enumerate(cost_param[:6]):\n",
    "    print(\"{}. Activation: {} | Objective Function: {} | N_layers: {} | N_hidden: {} | Epochs: {} | F1_Score: {}\".format(\\\n",
    "                                                                                                i+1,\n",
    "                                                                                                key[1]['phi'],\\\n",
    "                                                                                                key[1]['cost'],\\\n",
    "                                                                                                key[1]['nlayers'],\\\n",
    "                                                                                                key[1]['n_hidden'],\\\n",
    "                                                                                                key[1]['epochs'],\\\n",
    "                                                                                                key[0]\n",
    "                                                                                                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 300/300"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9416666666666667\n"
     ]
    }
   ],
   "source": [
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=300, # iterations\n",
    "              eta= 0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              nlayers=3,\n",
    "              phi='sigmoid',\n",
    "              cost='cross_entropy',\n",
    "              )\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=True)\n",
    "yhat = mlp.predict(X_test)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "for i,grad in enumerate(mlp.grad_avgs):\n",
    "    w = 'w{}'.format(i)\n",
    "    plt.plot(abs(grad[10:]), label=w)\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
